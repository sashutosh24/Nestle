# -*- coding: utf-8 -*-
"""(Youtube)Nestle sentimental Anlysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-BWO1ws6-Sq1xBmLjAmMX9sRs41xWvSm
"""

pip install google-api-python-client textblob pandas nltk

from googleapiclient.discovery import build
from textblob import TextBlob
import pandas as pd
import nltk
from collections import Counter

# Download NLTK stopwords (for analyzing concerns)
nltk.download('stopwords')
from nltk.corpus import stopwords

# Define API credentials
API_KEY = 'AIzaSyBfi3HVFk2J4CVfZqAbJaf118qJ0QCWeoM'  # Your API key
VIDEO_ID = 'rj6JOKrL_vg'  # Your video ID
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

# Function to fetch YouTube video comments
def fetch_comments(video_id, max_results=100):
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)
    comments = []
    try:
        response = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            maxResults=max_results,
            textFormat='plainText'
        ).execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
    except Exception as e:
        print(f"Error fetching comments: {e}")

    return comments

# Function to analyze sentiments
def analyze_sentiments(comments):
    sentiment_data = []
    for comment in comments:
        blob = TextBlob(comment)
        sentiment_score = blob.sentiment.polarity  # Polarity: -1 (negative) to +1 (positive)
        sentiment = 'Positive' if sentiment_score > 0 else 'Negative' if sentiment_score < 0 else 'Neutral'
        sentiment_data.append({
            'Comment': comment,
            'Polarity': sentiment_score,
            'Sentiment': sentiment
        })
    return pd.DataFrame(sentiment_data)

# Function to identify key concerns
def extract_concerns(comments):
    stop_words = set(stopwords.words('english'))
    keywords = []

    for comment in comments:
        words = comment.lower().split()
        keywords.extend([word for word in words if word not in stop_words and len(word) > 3])

    # Find the most common keywords
    keyword_counts = Counter(keywords)
    return keyword_counts.most_common(10)

# Main Execution
if __name__ == '__main__':
    print("Fetching comments...")
    comments = fetch_comments(VIDEO_ID, max_results=50)

    if comments:
        print(f"Fetched {len(comments)} comments. Analyzing sentiments...")
        sentiment_df = analyze_sentiments(comments)
        print(sentiment_df)

        # Save sentiment results to a CSV
        sentiment_df.to_csv('youtube_comments_sentiment_analysis.csv', index=False)
        print("Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.")

        print("\nExtracting customer concerns...")
        concerns = extract_concerns(comments)
        print("Top customer concerns:", concerns)

        # Save concerns to a CSV
        concerns_df = pd.DataFrame(concerns, columns=['Keyword', 'Frequency'])
        concerns_df.to_csv('customer_concerns.csv', index=False)
        print("Customer concerns saved to 'customer_concerns.csv'.")
    else:
        print("No comments fetched.")

from googleapiclient.discovery import build
from textblob import TextBlob
import pandas as pd
import nltk
from collections import Counter
from nltk.corpus import stopwords

# Download NLTK stopwords (for analyzing concerns)
nltk.download('stopwords')

# Define API credentials (replace with your actual API key and video ID)
API_KEY = 'YOUR_API_KEY'
VIDEO_ID = 'YOUR_VIDEO_ID'
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

# ... (rest of the code remains the same)

import pandas as pd

def analyze_nestle_concerns(csv_filepath):
    try:
        df = pd.read_csv(csv_filepath)
        keywords = df['Keyword'].tolist()

        # Basic analysis (expand for more insights)
        print("Top keywords related to negative sentiment towards Nestle:")
        for keyword in keywords:
          print(keyword)

    except FileNotFoundError:
        print(f"Error: File '{csv_filepath}' not found.")
    except Exception as e:
        print(f"An error occurred: {e}")


# Example usage (replace with your actual CSV file)
analyze_nestle_concerns('customer_concerns.csv')

from googleapiclient.discovery import build
from textblob import TextBlob
import pandas as pd
import nltk
from collections import Counter
from nltk.corpus import stopwords

# Install necessary libraries
!pip install google-api-python-client textblob pandas nltk

# Download NLTK stopwords (for analyzing concerns)
nltk.download('stopwords')

# Define API credentials
API_KEY = 'AIzaSyBfi3HVFk2J4CVfZqAbJaf118qJ0QCWeoM'  # Your API key
VIDEO_ID = 'rj6JOKrL_vg'  # Your video ID
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

# Function to fetch YouTube video comments
def fetch_comments(video_id, max_results=100):
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)
    comments = []
    try:
        response = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            maxResults=max_results,
            textFormat='plainText'
        ).execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
    except Exception as e:
        print(f"Error fetching comments: {e}")

    return comments

# Function to analyze sentiments
def analyze_sentiments(comments):
    sentiment_data = []
    for comment in comments:
        blob = TextBlob(comment)
        sentiment_score = blob.sentiment.polarity  # Polarity: -1 (negative) to +1 (positive)
        sentiment = 'Positive' if sentiment_score > 0 else 'Negative' if sentiment_score < 0 else 'Neutral'
        sentiment_data.append({
            'Comment': comment,
            'Polarity': sentiment_score,
            'Sentiment': sentiment
        })
    return pd.DataFrame(sentiment_data)

# Function to identify key concerns
def extract_concerns(comments):
    stop_words = set(stopwords.words('english'))
    keywords = []

    for comment in comments:
        words = comment.lower().split()
        keywords.extend([word for word in words if word not in stop_words and len(word) > 3])

    # Find the most common keywords
    keyword_counts = Counter(keywords)
    return keyword_counts.most_common(10)

# Main Execution
if __name__ == '__main__':
    print("Fetching comments...")
    comments = fetch_comments(VIDEO_ID, max_results=50)

    if comments:
        print(f"Fetched {len(comments)} comments. Analyzing sentiments...")
        sentiment_df = analyze_sentiments(comments)
        print(sentiment_df)

        # Save sentiment results to a CSV
        sentiment_df.to_csv('youtube_comments_sentiment_analysis.csv', index=False)
        print("Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.")

        print("\nExtracting customer concerns...")
        concerns = extract_concerns(comments)
        print("Top customer concerns:", concerns)

        # Save concerns to a CSV
        concerns_df = pd.DataFrame(concerns, columns=['Keyword', 'Frequency'])
        concerns_df.to_csv('customer_concerns.csv', index=False)
        print("Customer concerns saved to 'customer_concerns.csv'.")
    else:
        print("No comments fetched.")


def analyze_nestle_concerns(csv_filepath):
    try:
        df = pd.read_csv(csv_filepath)
        keywords = df['Keyword'].tolist()

        # Basic analysis (expand for more insights)
        print("Top keywords related to negative sentiment towards Nestle:")
        for keyword in keywords:
          print(keyword)

    except FileNotFoundError:
        print(f"Error: File '{csv_filepath}' not found.")
    except Exception as e:
        print(f"An error occurred: {e}")


# Example usage
analyze_nestle_concerns('customer_concerns.csv')

# prompt: I dont want keywords and frequency , I want to understand what it signifies by using NLP please help me in that way and give indepth what we need to change

from googleapiclient.discovery import build
from textblob import TextBlob
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer # For improved sentiment analysis
from nltk.stem import WordNetLemmatizer # For lemmatization
from collections import Counter

# Download necessary NLTK data
nltk.download('vader_lexicon')
nltk.download('wordnet')
nltk.download('omw-1.4') # Open Multilingual Wordnet
nltk.download('stopwords')


# ... (Your API key and video ID)

# Improved Sentiment Analysis
def analyze_sentiments(comments):
    analyzer = SentimentIntensityAnalyzer()
    sentiment_data = []
    for comment in comments:
        scores = analyzer.polarity_scores(comment)
        compound_score = scores['compound']  # Compound score: -1 to +1
        sentiment = 'Positive' if compound_score >= 0.05 else 'Negative' if compound_score <= -0.05 else 'Neutral'
        sentiment_data.append({
            'Comment': comment,
            'Compound Score': compound_score,
            'Sentiment': sentiment
        })
    return pd.DataFrame(sentiment_data)

# Enhanced Concern Extraction with Lemmatization
def extract_concerns(comments, sentiment_df):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    keywords = []

    # Only consider negative comments
    negative_comments = sentiment_df[sentiment_df['Sentiment'] == 'Negative']['Comment'].tolist()

    for comment in negative_comments:
        words = comment.lower().split()
        keywords.extend([lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 3])

    keyword_counts = Counter(keywords)
    return keyword_counts.most_common(10)  # Return top 10 concerns


# ... (rest of your code - fetch_comments remains the same)


if __name__ == '__main__':
    # ... (fetch comments)

    if comments:
        sentiment_df = analyze_sentiments(comments)
        # ... (Save sentiment analysis)

        print("\nExtracting customer concerns (from negative comments)...")
        concerns = extract_concerns(comments, sentiment_df)  # Pass sentiment dataframe

        # Analyze and print concerns
        if concerns:
          print("Top customer concerns:")
          for keyword, count in concerns:
              print(f"- {keyword} (Count: {count})")
              # Further analysis - categorize keywords, etc.
        else:
            print("No significant negative concerns found.")

        # ... (Save concerns to CSV)

# Enhanced Concern Extraction with Lemmatization
def extract_concerns(comments, sentiment_df):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    keywords = []

    # Only consider negative comments
    negative_comments = sentiment_df[sentiment_df['Sentiment'] == 'Negative']['Comment'].tolist()

    for comment in negative_comments:
        words = comment.lower().split()
        keywords.extend([lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 3])

    keyword_counts = Counter(keywords)
    return keyword_counts.most_common(10)  # Return top 10 concerns


# ... (rest of your code - fetch_comments remains the same)


if __name__ == '__main__':
    # ... (fetch comments)

    if comments:
        sentiment_df = analyze_sentiments(comments)
        # ... (Save sentiment analysis)

        print("\nExtracting customer concerns (from negative comments)...")
        concerns = extract_concerns(comments, sentiment_df)  # Pass sentiment dataframe

        # Analyze and print concerns
        if concerns:
          print("Top customer concerns:")
          for keyword, count in concerns:
              print(f"- {keyword} (Count: {count})")
              # Further analysis - categorize keywords, etc.
        else:
            print("No significant negative concerns found.")

        # ... (Save concerns to CSV)